{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7lOUxxbuurtE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install transformers datasets --quiet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets --quiet\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "1775c8343ff040d3b2f9d3ef63d60864",
            "52f1f56214434b9abb17925e40f8cc07",
            "f9884a6668ec4c389b80b791e07160f8",
            "30e8c8e911f147fd8a90b2fd0a0ba216",
            "8b94ac6264ce4d058610d64e2b8e4ccf",
            "6ad4a4cd7df743cd9a8edd40a0459ed7",
            "df3c3d8648c346778bb1be0741824258",
            "9637e7fa9ae24751a6742c3929eef6a2",
            "90f38a06883347948db7b4a6f5c9d65b",
            "b949f895f47445ccb797d27e50225c70",
            "11b1bfaa8eee401db7d9203369a6adb2",
            "6ff530640ddf411ea5dc4f9396ee24ad",
            "c2ccd4d93e8a4461ad84e172d028eaed",
            "212a9a44af5b45f68c40292c359bd438",
            "eab34a324bac49b5bc2402a34f840575",
            "b376c965520e4c729d3c5c1ed0445a24",
            "aebb4839ef4240848289a5137614214d",
            "3f69b6f715f44ae894d24a72d4fae1ab",
            "e31a0491bcf347d7b4320d4852a0a6f9",
            "f112203e8f9a4f409ac76f321b5849be",
            "d2ba84826e36417b93e0f6061f094559",
            "3d218fc3bb8d48028d56b8625c330716",
            "a045e0b1ab0b42189c86141e27f49749",
            "165993e6a6084033b4f8665398d2155b",
            "2d82830d2e8842d8bf26642f1545f623",
            "b8e3f283a2714b80b3bbc56bb92ff857",
            "051998c394e7473a87ab8ed9c3d52453",
            "dc1231e7df2449d5826ec5e82166b09f",
            "06b290dcb7ce4eb9b61248dd5b02bf68",
            "a485c5ad75a4419aa8bee181ed58b87e",
            "8b85154d0bca4f0ea7db0aa2533a5db4",
            "c1826d12207b4c12983350a4e0f19071",
            "fa4a76817d39479082237df110bebceb",
            "090a8d322670492fbbb2b7a9feec8d1c",
            "7e5fc55a140d4e25aa2f209536a7d9fd",
            "e4748112b9854b2b886a55ede7c76d48",
            "39a2e852953c4ea8a2e59c0af9cdc580",
            "4bd236d37cc5445283eed153122e54b9",
            "8816209566f549e2a23f6d4090e8c27c",
            "39d63fbeabe04a1fb93dd3b6d7fcd2a5",
            "729a8fe4832c459996a0c791e484dbd8",
            "277de8ddfed14004bb020ec3df4f3a52",
            "94cd530d2b7e45a0ad44d1319a6345a8",
            "11fa8f8936ca41a1ba0b50a4c1dc6e17",
            "6f2393331f0f4c0a8253927aba52a84e",
            "7ee4807954a2420ba9632a01e6d5f53d",
            "f38868138d5a4b1eb80e9f2db9acb244",
            "b1748a5816434f809acfd49fe3df748f",
            "2a64c397611e4f2eadf11d796481485d",
            "16e799d13ed24dd081cb5e1f9cdd371b",
            "0273da4a7ae54cccb9e960ddea0d07d9",
            "b59e5a9de1634374a721bac2d0a1dc81",
            "f193c2f7820b43feaf9260816e38c68f",
            "874f3d858c2a4c4a9806ef62fc59e449",
            "2b4330655c38451f81052e8658f352a7"
          ]
        },
        "id": "VBYOH8r_67lo",
        "outputId": "23b06df5-5f4c-43be-e5a7-27cfa6681d66"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1775c8343ff040d3b2f9d3ef63d60864",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ff530640ddf411ea5dc4f9396ee24ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a045e0b1ab0b42189c86141e27f49749",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "090a8d322670492fbbb2b7a9feec8d1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f2393331f0f4c0a8253927aba52a84e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = load_dataset('ag_news', split='train[:1000]')  # Small subset for demo\n",
        "\n",
        "texts = [item['text'] for item in dataset]\n",
        "with open(\"train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in texts:\n",
        "        f.write(line + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_2HrcuC7AJG"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def generate_few_shot(prompt, max_length=500):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    sample_output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return tokenizer.decode(sample_output[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS6Igme57IDa",
        "outputId": "798740ca-03ad-4c1c-b62d-1114c9b7e697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== FEW-SHOT (NO FINE-TUNING) ====\n",
            "Headline: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
            "Headline: Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
            "Headline: Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
            "Headline: The U.S. Economy (Reuters) Reuters - The U.S. economy is expected to be\\stronger than expected in the second quarter of the year, according to a report by the Federal Reserve.\n",
            "Headline: The U.S. Economy (Reuters) Reuters - The U.S. economy is expected to be\\stronger than expected in the second quarter of the year, according to a report by the Federal Reserve.\n",
            "Headline: The U.S. Economy (Reuters) Reuters - The U.S. economy is expected to be\\stronger than expected in the second quarter of the year, according to a report by the Federal Reserve.\n",
            "Headline: The U.S. Economy (Reuters) Reuters - The U.S. economy is expected to be\\stronger than expected in the second quarter of the year, according to a report by the Federal Reserve.\n",
            "Headline: The U.S. Economy (Reuters) Reuters - The U.S. economy is expected to be\\stronger than expected in the second quarter of the year, according to a report by the Federal Reserve.\n",
            "Headline: The U.S. Economy (Reuters) Reuters - The U.S. economy is expected to be\\stronger than expected in the second quarter of the year, according to a report by the Federal Reserve.\n",
            "Headline: The U.S. Economy (Reuters) Reuters - The U.S. economy is expected to be\\stronger than expected in the second quarter of the year, according to a report by the Federal Reserve.\n",
            "Headline: The U.S. Economy (Reuters) Reuters - The U.\n"
          ]
        }
      ],
      "source": [
        "few_shot_prompt = f\"\"\"Headline: {texts[0]}\n",
        "Headline: {texts[1]}\n",
        "Headline: {texts[2]}\n",
        "Headline:\"\"\"\n",
        "\n",
        "# Test few-shot completion\n",
        "print(\"==== FEW-SHOT (NO FINE-TUNING) ====\")\n",
        "print(generate_few_shot(few_shot_prompt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4byP-olC78RE",
        "outputId": "e3d81310-2962-44b3-8454-9008bf9eb2b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "def load_dataset_for_lm(file_path, tokenizer, block_size=128):\n",
        "    return TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=file_path,\n",
        "        block_size=block_size\n",
        "    )\n",
        "\n",
        "train_dataset = load_dataset_for_lm(\"train.txt\", tokenizer)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "PO5dM2vt8A7x",
        "outputId": "91be3d1d-e1ea-42f8-fa6d-2f64c534dd21"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='440' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [440/440 37:33, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.953100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.758600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>3.367400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>3.195700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('./gpt2-finetuned/tokenizer_config.json',\n",
              " './gpt2-finetuned/special_tokens_map.json',\n",
              " './gpt2-finetuned/vocab.json',\n",
              " './gpt2-finetuned/merges.txt',\n",
              " './gpt2-finetuned/added_tokens.json')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    do_train=True,\n",
        "    do_eval=False,\n",
        "    prediction_loss_only=True,\n",
        "    fp16=False\n",
        ")\n",
        "\n",
        "finetune_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=finetune_model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "finetune_model.save_pretrained(\"./gpt2-finetuned\")\n",
        "tokenizer.save_pretrained(\"./gpt2-finetuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOFaF1hscfIu"
      },
      "outputs": [],
      "source": [
        "# Load your fine-tuned model and tokenizer (do this once at the start after training)\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "ft_model = GPT2LMHeadModel.from_pretrained(\"./gpt2-finetuned\")\n",
        "ft_tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-finetuned\")\n",
        "\n",
        "def generate_finetuned(prompt, max_length=500):\n",
        "    input_ids = ft_tokenizer.encode(prompt, return_tensors='pt')\n",
        "    sample_output = ft_model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=ft_tokenizer.eos_token_id\n",
        "    )\n",
        "    return ft_tokenizer.decode(sample_output[0], skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "lBCEI3ZgblGD",
        "outputId": "958e62cf-bab7-457d-b20b-c6b5a1ec5b44"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "| Index | Prompt | Ground Truth | Few-Shot Output | Few-Shot BLEU | Fine-Tuned Output | Fine-Tuned BLEU |\n",
              "|---|---|---|---|---|---|---|\n",
              "| 0 | Headline: Non-OPEC Nations Should Up Output-Purnomo  JAKARTA... | Google IPO Auction Off to Rocky Start  WASHINGTON/NEW YORK (... | Headline: Non-OPEC Nations Should Up Output-Purnomo  JAKARTA... | 0.00 | Headline: Non-OPEC Nations Should Up Output-Purnomo  JAKARTA... | 0.00 |\n",
              "| 1 | Headline: Indians fill rail skills shortage Network Rail fli... | Steady as they go BEDFORD -- Scientists at NitroMed Inc. hop... | Headline: Indians fill rail skills shortage Network Rail fli... | 0.00 | Headline: Indians fill rail skills shortage Network Rail fli... | 0.00 |\n",
              "| 2 | Headline: Stoking the Steamroller No other recording artist ... | Coming to The Rescue Got a unique problem? Not to worry: you... | Headline: Stoking the Steamroller No other recording artist ... | 0.00 | Headline: Stoking the Steamroller No other recording artist ... | 0.00 |\n",
              "| 3 | Headline: 'Madden,' 'ESPN' Football Score in Different Ways ... | Group to Propose New High-Speed Wireless Format (Reuters) Re... | Headline: 'Madden,' 'ESPN' Football Score in Different Ways ... | 0.00 | Headline: 'Madden,' 'ESPN' Football Score in Different Ways ... | 0.01 |\n",
              "| 4 | Headline: Science, Politics Collide in Election Year (AP) AP... | Building Dedicated to Columbia Astronauts (AP) AP - A former... | Headline: Science, Politics Collide in Election Year (AP) AP... | 0.00 | Headline: Science, Politics Collide in Election Year (AP) AP... | 0.00 |\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ===================================\n",
        "# Model Comparison (Context: use after previous notebook sections)\n",
        "# ===================================\n",
        "\n",
        "# Let's compare both models over several samples for a fair evaluation.\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_bleu(reference, candidate):\n",
        "    \"\"\"\n",
        "    Compute BLEU score between a reference string and a candidate string.\n",
        "    \"\"\"\n",
        "    reference = [reference.split()]\n",
        "    candidate = candidate.split()\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    return sentence_bleu(reference, candidate, smoothing_function=smoothie)\n",
        "\n",
        "# Select several test indices\n",
        "test_indices = [12, 34, 56, 78, 90]  # change as appropriate for your dataset size\n",
        "\n",
        "# Store results for display\n",
        "results = []\n",
        "for idx in test_indices:\n",
        "    # Prompt using N-1 context, or just single line\n",
        "    prompt = \"Headline: \" + texts[idx] + \"\\nHeadline:\"\n",
        "    ground_truth = texts[idx + 1]\n",
        "    few_shot_output = generate_few_shot(prompt)\n",
        "    fine_tuned_output = generate_finetuned(prompt)\n",
        "    few_shot_bleu = get_bleu(ground_truth, few_shot_output)\n",
        "    fine_tuned_bleu = get_bleu(ground_truth, fine_tuned_output)\n",
        "    results.append({\n",
        "        'Prompt': prompt,\n",
        "        'Ground Truth': ground_truth,\n",
        "        'Few-Shot Output': few_shot_output,\n",
        "        'Few-Shot BLEU': few_shot_bleu,\n",
        "        'Fine-Tuned Output': fine_tuned_output,\n",
        "        'Fine-Tuned BLEU': fine_tuned_bleu\n",
        "    })\n",
        "\n",
        "# Display results in a markdown-style table\n",
        "def display_results_table(results):\n",
        "    table = \"| Index | Prompt | Ground Truth | Few-Shot Output | Few-Shot BLEU | Fine-Tuned Output | Fine-Tuned BLEU |\\n\"\n",
        "    table += \"|---|---|---|---|---|---|---|\\n\"\n",
        "    for i, row in enumerate(results):\n",
        "        # Show just first 60 chars of prompt/outputs for compactness\n",
        "        table += f\"| {i} | {row['Prompt'][:60]}... | {row['Ground Truth'][:60]}... | {row['Few-Shot Output'][:60]}... | {row['Few-Shot BLEU']:.2f} | {row['Fine-Tuned Output'][:60]}... | {row['Fine-Tuned BLEU']:.2f} |\\n\"\n",
        "    display(Markdown(table))\n",
        "\n",
        "display_results_table(results)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
