# GPT-2 Fine-tuning on AG News Headlines

This project demonstrates how to **finetune OpenAI's GPT-2 model** on the AG News dataset using Hugging Face Transformers and Datasets. It includes both few-shot (prompt-only) and supervised finetuning strategies with side-by-side evaluation.

---

## Table of Contents

- [Project Overview](#project-overview)
- [Setup & Installation](#setup--installation)
- [Usage](#usage)
- [Dataset](#dataset)
- [Project Structure](#project-structure)
- [Results](#results)
- [Acknowledgments](#acknowledgments)

---

## Project Overview

This repository provides a full pipeline to download, process, and finetune GPT-2 on AG News headlines. You can compare few-shot prompting with true finetuned results using automated BLEU metrics and sample generations.

---

## Setup & Installation

1. **Clone this repository**  

2. **Install requirements**  
*(Recommended: Python 3.8+)*

3. (Optional) For BLEU evaluation: pip install nltk


---

## Usage

1. **Run the notebook**  
- Open `gpt2_finetuning_agnews.ipynb` in Jupyter, Colab, or VS Code.
- Follow the cells in order: this downloads AG News, processes data, loads GPT-2, runs few-shot baseline, then trains and evaluates the finetuned model.

2. **Key steps:**  
- Download & preprocess AG News.
- Generate with few-shot prompt only.
- Finetune GPT-2 using Hugging Face Trainer.
- Evaluate generations using BLEU; view results.

---

## Dataset

- **AG News** ([Kaggle](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset))
- Uses only the 'train' split (subset for demo).
- Preprocessing: headlines extracted, stored to `train.txt`.

---

## Project Structure

| File                         | Purpose                                      |
|------------------------------|----------------------------------------------|
| gpt2_finetuning_agnews.ipynb | Full pipeline: preprocess, train, evaluate   |
| train.txt                    | Plain text training set (autogenerated)      |
| ./gpt2-finetuned/            | Saved finetuned model outputs                |

---

## Results

Sample BLEU evaluation table and output comparisons are printed at the end of the notebook. See the last section for example markdown results and model generations.

---

## Acknowledgments

- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [AG News Dataset](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset)
- Techniques inspired by open source NLP fine-tuning recipes.

---


