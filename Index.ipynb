{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AceOfSpades-1710/GPT2-FineTuning/blob/main/Index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7lOUxxbuurtE"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets --quiet\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('ag_news', split='train[:1000]')  # Small subset for demo\n",
        "\n",
        "texts = [item['text'] for item in dataset]\n",
        "with open(\"train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in texts:\n",
        "        f.write(line + '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "76aa56faf50f409b8dde9e81f6c685fb",
            "522b4622d1ca4c3da6539db3964f43eb",
            "0b235a9615c547faa4e86aa44553b611",
            "2d01df2cffb44d9d85e2448ba18a4e7a",
            "a4e04734a3fc4b73a609ad087ba77d49",
            "e91bba65eb5e47e1ae11c0a6dbfc50dd",
            "d16fb4fba5a34600a14ab709f5854904",
            "5f7a9e5c82614f9a8e4b9b9965df84b4",
            "46532bd1d15e49d9a33d76f22abdbcaa",
            "4db38bd29d8748bfba026a0dfa1d629c",
            "e4a861aedd6b419b92c5dbd9d22dffac",
            "bd8546f788994367926dea8cc8c128be",
            "637628661d6244e2af28d863d6f73130",
            "0c6be3c320aa408aadf456392b38762c",
            "a1a473c0c0ca48419aba53c46a1525f2",
            "0213f47321214ecdad13cf78556b2f8b",
            "c3b774024d904166b31a4dd78d0eee46",
            "8f085da25c104ef7a5894025fe8ecb34",
            "6f9bcb6454034a298de2962a394ef981",
            "2f44b15169a84e48ab093e6000b18d41",
            "cf87d055e38b4366a17af7b7eb18aba0",
            "6b6536faec2f4997b584d8c8778d8a93",
            "c3c7ef8e9ea24bb2a5fd1b7c905e9706",
            "5280cb7335184031823205202cd93836",
            "da1ebb1e1cb743a39024e71c46ef6d17",
            "7311270aa7d140b1b6ac9de9af224228",
            "f70a5a2e47fe4e07acec00867cb97559",
            "f9870c520c114036bf109cba514cb615",
            "3479e59968094104ae454ddb3898e1fa",
            "5dcc5df34a004a5fb4df377d08045e48",
            "11cc2647ca194d07b2f1df05fd3f9ea3",
            "1a5a9deadd1c4859833006a0418e0280",
            "767fbf4fe3ae4535a4353a7cc1fa8e4f",
            "b1c372d0e53f4494a104b9db9946a4ab",
            "e48679e4668a478f955168a30f9b89c5",
            "36bf15ff33a74c988a5ceb350a1579b8",
            "96a0104b6cc8420fb51a4a1c55bee7d4",
            "2caf61b26bfc4226afd5f97a64614bc3",
            "c64c6d42abae4860b7881448e9ed91d8",
            "0dffb8fac3b74f4e9ffee07a6fe4db69",
            "c95e1db52128468ab322c88da173d5f9",
            "2e8e219aadb248879c33f7af084cb401",
            "15e2b6cb771a47bd966adbe758616e32",
            "8737a19259ef4d34a6fd70a20511f357",
            "f3211dc1fc8c47a5a91d6efba699ecf5",
            "8748227972ef4a289818aacc6266bb63",
            "db99b7a3df6549a09229806df4f5278e",
            "d1bd4d9feb4149958aaca7085deaba15",
            "a87944a28e6c4a3c8e4fc63591f59860",
            "799248f87b0848c98cf70a9280353247",
            "09df658a1dd54964b6d76b4ac3c10744",
            "0c64ca03590e4025999b00f58cc61e1d",
            "d6938a26acff47698eb3c0d2c0bf8ac4",
            "6940127d43b2406cabbdcedbf971e024",
            "be07889096504f5eb220ddd1732ebc0b"
          ]
        },
        "id": "VBYOH8r_67lo",
        "outputId": "fab66f06-88ba-4f6a-fdc5-56cfad4a2cd6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76aa56faf50f409b8dde9e81f6c685fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd8546f788994367926dea8cc8c128be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3c7ef8e9ea24bb2a5fd1b7c905e9706"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1c372d0e53f4494a104b9db9946a4ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3211dc1fc8c47a5a91d6efba699ecf5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def generate_few_shot(prompt, max_length=500):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    sample_output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return tokenizer.decode(sample_output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "o_2HrcuC7AJG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241,
          "referenced_widgets": [
            "1df7659aac61451a9f3b1b7c07580039",
            "665fbd7fc10e4c519cc468d2481560a5",
            "a539951e67a24154918b5cbce12beac9",
            "9e0371f5562a4e52b56a9ccf97b470fe",
            "1d177cfa02b04f17a5012ccffd41d5aa",
            "ec2f393877f244b4acaecf74b86cbffa",
            "9e51d0c9cc0d4343b2b4f8a077d29f73",
            "38f3b2d0f0884da9a4b6f734e9b44539",
            "f4f1923418de454cb14415ab151950eb",
            "5803032a84fe43babf32abfe94327ac4",
            "b5ad5bb360b84fecb1504c068708e6f1",
            "f5947b91526047978d5bf55f11944787",
            "d1b7e7acd5ca47218e44d8f52b21e100",
            "ef8831604c474817b068443bd79245c2",
            "e4318aca163340729726e0e9bcdee314",
            "44fbded5e86948ea9657225a6b683a60",
            "7a4f37897fe74e798f319635783418e9",
            "d63ae3e10fe64ab98d51d8a1cae8b67e",
            "105e2b8f0ba14c50a5c5b195dc8e08e5",
            "add48b333ba1400585d225356455f3a0",
            "f439d994f6b34d97be391eefc92efe6c",
            "7d62b00b286d4fe587bfc355e7c5e734",
            "d0b412b6ea664588be192f4b33a06ed8",
            "63e779b73c3340aba026ffcdcc502e16",
            "e6b023e7d291492eb572a57fbf31d464",
            "cb477a5a4a4341da9e0c8159d38e76d8",
            "3d6a7ab0e519489a92cdf04b180df110",
            "135787d95466496ea4c768d804b0c2c4",
            "4e4a51ca29a4422cb35c5afafb7e6c05",
            "519c3d3fca4a47598b489a47c232f82b",
            "b2b6ab3397d840d5aa53dc54de9ff24f",
            "49c45f250f5548498f2dcd9c0ce58edf",
            "71198ca0d7fe4c799e515e50e3ce740f",
            "6b0c465106db41a09de9de0f867ab73f",
            "1f3a808c01c943a59845539a08ca34fd",
            "a17fa7a0b03042a8bc1a0882308629b1",
            "51e7149d713e47e89ceaf6d935683b7d",
            "bdddd4ae5ebf48daa2e1bb64c2e1cd82",
            "99354fd47b28426b9c7ac045bdc6ed65",
            "cf8890027b1f48a785ffe516ae6a35e8",
            "644bbecddad243019f822f7c34c22bb4",
            "8bdc50c7f1264475bc4ee38fcd87459f",
            "7af1f30d029044748983d6f8b250f98e",
            "bb867baa36ad4f79a662d991c799df3a",
            "f6718e9384924c79a0462c1d7d2efa1e",
            "f38067f12caf4f8da6e1c49906a19566",
            "900fbaf373e149f4a047ca736b8d134b",
            "84e226aebfd841cead049c4b714c077b",
            "c3823afd365b4906b4b4e49bd8ad5315",
            "d01f3b36fe23441e8a091266efbd478a",
            "bd215ffd49234370902c205fc721087a",
            "7fcd1ad9376e41e3a60023b083a49680",
            "407dedb2731e46d8984b0a4d08b8084a",
            "ceb3fb3f1984481899b490ed232437d8",
            "8d0c2cb275e647de800ffee31f840374",
            "f980dcc35e964ef3b108c55ad31dce70",
            "e48ecebc320e46e084d5b02cde02a89f",
            "b3ae0337020d4d50993989e7bba55c72",
            "565f7eb8b9574dcea90b38e39227bf27",
            "567a8236b95c475eb095bbf6417a5eef",
            "ab69ae867d96406dac81ea94f12e5919",
            "8a1e4d157a8d438d82df66119f2e4775",
            "fca9808d055949678a66b64cbe420e75",
            "747fda979d7c4ae9ba3aec83875aa9ba",
            "063239611f0642ae8560c833f8c369ec",
            "376bfad51c624b50886fd4ddac4dccc8",
            "56b301ba5a4c4be283a9c549937fe9d7",
            "8865c14f20f745699cc7a232be58aa15",
            "2a194cce018645078217c32e8f641016",
            "d6977a4046954c85ac61dbffde7cd2a9",
            "e9f7f2f3e4c643e885d6572212f1ca44",
            "0fa92ccd4ef941cfbe123627e5d5db10",
            "55ecbcc0abc04d64882275fd8cf58458",
            "aab5998f7c6c4bfe9fefe6168e5df3be",
            "fc33f5296d394e3ea6bbbeb4490f186d",
            "47f1daea317c4c25b243240ca1d59a8c",
            "34f3b6f3be494a6db900a7bea470d63b"
          ]
        },
        "outputId": "882c3229-3033-4d1c-f0bf-4a9c10a809d3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1df7659aac61451a9f3b1b7c07580039"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5947b91526047978d5bf55f11944787"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0b412b6ea664588be192f4b33a06ed8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b0c465106db41a09de9de0f867ab73f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6718e9384924c79a0462c1d7d2efa1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f980dcc35e964ef3b108c55ad31dce70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56b301ba5a4c4be283a9c549937fe9d7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = f\"\"\"Headline: {texts[0]}\n",
        "Headline: {texts[1]}\n",
        "Headline: {texts[2]}\n",
        "Headline:\"\"\"\n",
        "\n",
        "# Test few-shot completion\n",
        "print(\"==== FEW-SHOT (NO FINE-TUNING) ====\")\n",
        "print(generate_few_shot(few_shot_prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS6Igme57IDa",
        "outputId": "c5614955-7da3-49d1-990e-2b6758491ebe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== FEW-SHOT (NO FINE-TUNING) ====\n",
            "Headline: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
            "Headline: Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
            "Headline: Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
            "Headline: The U.S. Economy (Reuters) Reuters - The U.S. economy is expected to be\\stronger than expected in the second quarter of the year, according to a report by the Federal Reserve.\n",
            "Headline: The U.S. Economy (Reuters) Reuters - The U.S. economy is expected to be\\stronger than expected in the second quarter of the year, according to a report by the Federal Reserve.\n",
            "Headline: The U.S. Economy (Reuters) Reuters - The U.S. economy is expected to be\\stronger than expected in the second quarter of the year, according to a report by the Federal Reserve.\n",
            "Headline: The U.S. Economy (Reuters) Reuters - The U.S. economy is expected to be\\stronger than expected in the second quarter of the year, according to a report by the Federal Reserve.\n",
            "Headline: The U.S. Economy (Reuters) Reuters - The U.S. economy is expected to be\\stronger than expected in the second quarter of the year, according to a report by the Federal Reserve.\n",
            "Headline: The U.S. Economy (Reuters) Reuters - The U.S. economy is expected to be\\stronger than expected in the second quarter of the year, according to a report by the Federal Reserve.\n",
            "Headline: The U.S. Economy (Reuters) Reuters - The U.S. economy is expected to be\\stronger than expected in the second quarter of the year, according to a report by the Federal Reserve.\n",
            "Headline: The U.S. Economy (Reuters) Reuters - The U.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset_for_lm(file_path, tokenizer, block_size=128):\n",
        "    return TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=file_path,\n",
        "        block_size=block_size\n",
        "    )\n",
        "\n",
        "train_dataset = load_dataset_for_lm(\"train.txt\", tokenizer)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4byP-olC78RE",
        "outputId": "612a9a8f-e171-4663-9e5d-d28bab00c90c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the  Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    do_train=True,\n",
        "    do_eval=False,\n",
        "    prediction_loss_only=True,\n",
        "    fp16=False\n",
        ")\n",
        "\n",
        "finetune_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=finetune_model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "finetune_model.save_pretrained(\"./gpt2-finetuned\")\n",
        "tokenizer.save_pretrained(\"./gpt2-finetuned\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "PO5dM2vt8A7x",
        "outputId": "f3e2b24b-498c-44a3-8d76-0067d7fa0c96"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 路路路路路路路路路路\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthriftscrapper\u001b[0m (\u001b[33mthriftscrapper-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "creating run (0.0s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250914_063545-xwghyge9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/thriftscrapper-personal/huggingface/runs/xwghyge9' target=\"_blank\">likely-shadow-2</a></strong> to <a href='https://wandb.ai/thriftscrapper-personal/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/thriftscrapper-personal/huggingface' target=\"_blank\">https://wandb.ai/thriftscrapper-personal/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/thriftscrapper-personal/huggingface/runs/xwghyge9' target=\"_blank\">https://wandb.ai/thriftscrapper-personal/huggingface/runs/xwghyge9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='440' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [440/440 36:06, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.953100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.758600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>3.367400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>3.195700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./gpt2-finetuned/tokenizer_config.json',\n",
              " './gpt2-finetuned/special_tokens_map.json',\n",
              " './gpt2-finetuned/vocab.json',\n",
              " './gpt2-finetuned/merges.txt',\n",
              " './gpt2-finetuned/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your fine-tuned model and tokenizer (do this once at the start after training)\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "ft_model = GPT2LMHeadModel.from_pretrained(\"./gpt2-finetuned\")\n",
        "ft_tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-finetuned\")\n",
        "\n",
        "def generate_finetuned(prompt, max_length=500):\n",
        "    input_ids = ft_tokenizer.encode(prompt, return_tensors='pt')\n",
        "    sample_output = ft_model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=ft_tokenizer.eos_token_id\n",
        "    )\n",
        "    return ft_tokenizer.decode(sample_output[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "UOFaF1hscfIu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================\n",
        "# Model Comparison (Context: use after previous notebook sections)\n",
        "# ===================================\n",
        "\n",
        "# Let's compare both models over several samples for a fair evaluation.\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_bleu(reference, candidate):\n",
        "    \"\"\"\n",
        "    Compute BLEU score between a reference string and a candidate string.\n",
        "    \"\"\"\n",
        "    reference = [reference.split()]\n",
        "    candidate = candidate.split()\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    return sentence_bleu(reference, candidate, smoothing_function=smoothie)\n",
        "\n",
        "# Select several test indices\n",
        "test_indices = [12, 34, 56, 78, 90]  # change as appropriate for your dataset size\n",
        "\n",
        "# Store results for display\n",
        "results = []\n",
        "for idx in test_indices:\n",
        "    # Prompt using N-1 context, or just single line\n",
        "    prompt = \"Headline: \" + texts[idx] + \"\\nHeadline:\"\n",
        "    ground_truth = texts[idx + 1]\n",
        "    few_shot_output = generate_few_shot(prompt)\n",
        "    fine_tuned_output = generate_finetuned(prompt)\n",
        "    few_shot_bleu = get_bleu(ground_truth, few_shot_output)\n",
        "    fine_tuned_bleu = get_bleu(ground_truth, fine_tuned_output)\n",
        "    results.append({\n",
        "        'Prompt': prompt,\n",
        "        'Ground Truth': ground_truth,\n",
        "        'Few-Shot Output': few_shot_output,\n",
        "        'Few-Shot BLEU': few_shot_bleu,\n",
        "        'Fine-Tuned Output': fine_tuned_output,\n",
        "        'Fine-Tuned BLEU': fine_tuned_bleu\n",
        "    })\n",
        "\n",
        "# Display results in a markdown-style table\n",
        "def display_results_table(results):\n",
        "    table = \"| Index | Prompt | Ground Truth | Few-Shot Output | Few-Shot BLEU | Fine-Tuned Output | Fine-Tuned BLEU |\\n\"\n",
        "    table += \"|---|---|---|---|---|---|---|\\n\"\n",
        "    for i, row in enumerate(results):\n",
        "        # Show just first 60 chars of prompt/outputs for compactness\n",
        "        table += f\"| {i} | {row['Prompt'][:60]}... | {row['Ground Truth'][:60]}... | {row['Few-Shot Output'][:60]}... | {row['Few-Shot BLEU']:.2f} | {row['Fine-Tuned Output'][:60]}... | {row['Fine-Tuned BLEU']:.2f} |\\n\"\n",
        "    display(Markdown(table))\n",
        "\n",
        "display_results_table(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "lBCEI3ZgblGD",
        "outputId": "1ef4c146-3476-46c0-e1c3-9aac0260073b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "| Index | Prompt | Ground Truth | Few-Shot Output | Few-Shot BLEU | Fine-Tuned Output | Fine-Tuned BLEU |\n|---|---|---|---|---|---|---|\n| 0 | Headline: Non-OPEC Nations Should Up Output-Purnomo  JAKARTA... | Google IPO Auction Off to Rocky Start  WASHINGTON/NEW YORK (... | Headline: Non-OPEC Nations Should Up Output-Purnomo  JAKARTA... | 0.00 | Headline: Non-OPEC Nations Should Up Output-Purnomo  JAKARTA... | 0.00 |\n| 1 | Headline: Indians fill rail skills shortage Network Rail fli... | Steady as they go BEDFORD -- Scientists at NitroMed Inc. hop... | Headline: Indians fill rail skills shortage Network Rail fli... | 0.00 | Headline: Indians fill rail skills shortage Network Rail fli... | 0.00 |\n| 2 | Headline: Stoking the Steamroller No other recording artist ... | Coming to The Rescue Got a unique problem? Not to worry: you... | Headline: Stoking the Steamroller No other recording artist ... | 0.00 | Headline: Stoking the Steamroller No other recording artist ... | 0.00 |\n| 3 | Headline: 'Madden,' 'ESPN' Football Score in Different Ways ... | Group to Propose New High-Speed Wireless Format (Reuters) Re... | Headline: 'Madden,' 'ESPN' Football Score in Different Ways ... | 0.00 | Headline: 'Madden,' 'ESPN' Football Score in Different Ways ... | 0.01 |\n| 4 | Headline: Science, Politics Collide in Election Year (AP) AP... | Building Dedicated to Columbia Astronauts (AP) AP - A former... | Headline: Science, Politics Collide in Election Year (AP) AP... | 0.00 | Headline: Science, Politics Collide in Election Year (AP) AP... | 0.00 |\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMW0a7vHgDVLKAv3FJ2QX8f",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
